- 简述决策树的原理

  >  决策树是通过一系列规则对数据进行分类的过程。 决策树分为分类树和回归树。

-  简述决策树的构建过程 

  > 1. 特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准。
  > 2. 决策树生成： 根据选择的特征评估标准，从上至下递归地生成子节点，直到满足终止条件。
  > 3. 剪枝：防止过拟合。

- 决策树常用的几种算法如何划分特征？

  >  ID3使用的是信息熵增益选大的方法划分数据 
  >
  >  C4.5是使用增益率选大的方法划分数据 
  >
  >  CART使用的是基尼指数选小的划分方法 
  >  

- 三种算法划分特征的特点？

  > ID3： 
  >
  > ​		  以信息论为基础，以信息熵和信息增益度为衡量标准
  >
  > ​		  偏向于选择取值较多的属性  
  >
  > ​	     不能处理具有连续值的属性，也不能处理具有缺失数据的属性 
  >
  > C4.5： 
  >
  > ​			增益率的划分方法 
  >
  > ​			具有较高的准确率
  >
  > ​			在构造树的过程中进行剪枝，使用的是悲观剪枝法（使用错误率来评估） 
  >
  > ​	  	 效率比较低  
  >
  > ​			完成对连续属性的离散化处理 
  >
  > ​			能够对不完整数据进行处理 
  >
  > CART：
  >
  > ​		  分类树：基尼指数最小化
  >
  >  ​		  回归树：平方误差最小化  	
  >
  > ​		  使用基尼指数的划分准则；通过在每个步骤最大限度降低不纯洁度 
  >
  > ​		  能够处理孤立点以及能够对空缺值进行处理

- 对信息增益和信息增益率的理解？

  > 熵： 
  >
  > ​		对随机变量不确定性的度量（纯度），也可以说是对随机变量的概率分布的一个衡量。 
  >
  > ​		熵越大，随机变量的不确定性就越大。
  >
  > ​		对同一个随机变量，当他的概率分布为均匀分布时，不确定性最大，熵也最大。 
  >
  > ​		对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大。 
  >
  > ​		 当熵和条件熵中概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别为经验熵与经验条件熵。 
  >
  > 
  >
  > 信息增益：
  >
  > ​		  也叫互信息
  >
  > ​		  就是指集合D的经验熵与特征A给定条件下D的经验条件熵之差。
  >
  > ​			$g(D,A)=H(D)-H(D|A)$ 
  >
  > ​			信息增益准则对那些特征的取值比较多的特征有所偏好 (ID3中所存在的问题)
  >
  > ​			越小说明使用此特征划分得到的子集的不确定性越小（也就是纯度越高） 
  >
  > 
  >
  > 信息增益率：
  >
  > ​			$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$
  >
  > ​			惩罚参数：数据集D以特征A作为随机变量的熵的倒数
  >
  > ​			$H_A(D)$表示的就是特征A的纯度，如果A只含有少量的取值的话，那么A的纯度就比较高 
  >
  > ​			 信息增益比偏向取值较少的特征
  >
  > 
  >
  > 基尼指数：
  >
  > ​			$Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$		
  >
  > ​		    Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高 
  >
  > ​			基于特征A划分样本集合之后的基尼指数：
  >
  > ​			$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2) $

- 树划分的终止条件？

  > 1. 结点达到完全纯度
  > 2. 树的深度达到用户所要深度
  > 3. 结点中样本的数量属于用户指定的个数

- 剪枝？

  > - 先（预）剪枝：在构造过程中， 对划分前和划分后的泛化性能进行估计，当某个结点不满足剪枝条件，则直接停止此分支的构造。 
  >
  > ​		优点：预剪枝使得决策树的很多分支都没有展开，这不仅降低来过拟合的风险，还显著减少了决策树的训练时间开销和测试时间的开销。
  >
  > ​		缺点：预剪枝基于“贪心”的本质，禁止这些分支展开，给预剪枝决策树带来来欠拟合的风险。
  >
  > - 后剪枝：先构造完成完整的决策树， 从最后一个分支来判断要不要剪枝 。
  >
  >   优点：后剪枝策略通常比预剪枝策略保留更多的分支。一般情况下，后剪枝决策树的欠拟合风险很小，泛化能力往往优于预剪枝决策树
  >
  >   缺点：训练时间开销比未剪枝决策树和预剪枝决策树都要大的多。 
  >
  > 
  >
  > 剪枝过程中可以参考的参数：树的高度、叶子节点的数目、最大叶子节点数、限制不纯度。 

- 决策树解决过拟合的办法？（**TODO**）

  > - 剪枝
  > - 交叉验证
  > - 特征提取：踢出相关性较小的特征；通过正则化的方式来进行特征选取
  > - 随机森林

- 什么是决策树？

  > - 一颗树模型
  > -  if-then规则的集合 
  > -  定义在特征空间与类空间上的条件概率分布 

- 决策树的优缺点？

  > 优点：
  >
  > - 模型可解释性强
  > - 分类速度快
  > - 比较适合处理有缺失属性的样本（why?）
  >
  > 缺点：
  >
  > - 容易发生过拟合（随机森林可以很大程度上减少过拟合）
  > - 忽略了数据之间的相关性
  > -  信息增益准则对那些特征的取值比较多的特征有所偏好

- 决策树与逻辑回归的区别？

  > 1. 对于拥有**缺失值**的数据，决策树可以应对，而逻辑回归需要预先对缺失数据进行处理； 
  >
  > 2. 逻辑回归对数据**整体结构**的分析优于决策树，而决策树对**局部结构**的分析优于逻辑回归； （决策树由于采用分割的方法，所以能够深入数据内部，但同时失去了对全局的把握。一个分层一旦形成，它和别的层面或节点的关系就被切断了，以后的挖掘只能在局部中进行。同时由于切分，样本数量不断萎缩，所以无法支持对多变量的同时检验。而逻辑回归，始终着眼整个数据的拟合，所以对全局把握较好。但无法兼顾局部数据，或者说缺乏探查局部结构的内在机制。） 
  > 3. 逻辑回归擅长分析**线性关系**，而决策树对线性关系的把握较差。 
  >
  > 4. 逻辑回归对**极值**比较敏感，容易受极端值的影响，而决策树在这方面表现较好。 
  > 5. 应用上的区别：**决策树的结果和逻辑回归相比略显粗糙**。逻辑回归原则上可以提供数据中每个观察点的概率，而决策树只能把挖掘对象分为有限的概率组群。
  > 6. **执行速度**上：当数据量很大的时候，逻辑回归的执行速度非常慢，而决策树的运行速度明显快于逻辑回归。  

- 怎样处理连续值？

  > C4.5算法采用的是最简单的策略二分法来对连续值进行处理。把不同的样本的该连续属性值从小到大排序，然后找到候选划分点，把每个候选划分点的信息增益算出来，取max的值作为该属性的信息增益。
  >
  > 注意：与离散属性值不同，若当前结点划分属性为连续的，那么该属性还可以作为后面结点的划分属性。 

- 怎样处理缺失值？（**TODO**）

  > 1. 怎样处理在属性值缺失的情况下计算属性的信息增益，从而进行划分属性的选择？假答：设17个样本在“色泽”属性上只有14个是有值的，另外3个是缺失的，那么就以这14个样本的属性值计算信息增益，然后把这个值乘以14/17（？？应该是17/14吧），相当于乘上一个权重，当作这全部17个样本的信息增益。 	
  >
  > 2. 给定划分属性，如果样本在该属性值上是缺失的，如何对这个样本进行划分？ 
  >
  >    答：本来每个样本在结点中的权重都是1，当样本「8」在“纹理”上出现缺失值，那么在“纹理=清晰”、“纹理=模糊”、“纹理=稍糊”这三个分支中都会出现样本「8」，只是权重不再是1，而是7/15，5/15，3/15（这三个权重的来源是这三个分支中各个样本的比例，不包括这个样本「8」） 
  >

- 什么是多变量决策树

  > 在多变量决策树中，非叶子结点不再是针对某个属性，而是对属性的线性组合进行测试，也就是说每一个非叶子结点就是一个线性分类器。 