- 简述一下KNN算法的原理？

  > 近朱者赤近墨者黑： k 近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。
  >
  > 给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。 
  >
  > k近邻模型的三要素：距离度量、k值选择、分类决策规则

- KNN算法优点和缺点？ 

  > **优点**
  >
  > - 思想简单，理论成熟 
  > - KNN可以处理多分类问题和回归问题
  > - 可以用来非线性分类
  > - 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN方法较其他方法更为适合 
  > - 比较适用于样本容量比较大的类域的自动分类
  > - 准确度高，对数据没有假设，对离群值不敏感。 
  >
  > **缺点**
  >
  > - 效率低， 尤其是特征数非常多的时候 。因为每一次分类或者回归，都需要计算目标分类点与训练数据所有点之间的距离。
  > - 对训练数据依赖度大，当样本分布不均衡，比如其中一类样本过大，新的未知实例容易被归类到这个主导样本， 对稀有类别的预测准确率低 。如下图由于k近邻的点疏密程度不一样，若k值稍微大一点，则会导致分类错误
  > - 维数灾难，KNN对多维度的数据处理也不是很好。随着维度增加，看似相近的两个点之间的距离越来越大，就会越来越不像。
  > - KD树，球树之类的模型建立需要大量的内存 。
  > - 不具有显示的学习过程，导致预测时速度比起逻辑回归之类的算法慢 
  > - 相比决策树模型，可解释性不强

- 不平衡样本可以给KNN预测结果造成哪些问题，有没有什么好的解决方式？ 

  > 可能造成大数量的样本占多数，但是这类样本不接近目标样本。而数量小的这类样本很靠近目标样本。 
  >
  > **改进方法**：和该样本距离小的邻居权值大，远的权值小 （加权投票法）
  >
  > 最简单的方法就是1/distance , 比较好的方法是用**kernel weights**。

- 什么是kernel weights？

   <img src="assets\knn-kernal weights.png" alt="image-20191216173635385" style="zoom:50%;" />

    不同的kernel，weight的衰减不同。

    每种kernel都有 $\lambda$ 参数，叫做bandwidth。 

- 训练样本是否要一视同仁？

   >  在训练集中，有些样本可能是更值得依赖的。 
   >
   >  可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响。 

- 如何加快分类速度？

  > 1. 减少样本量： 可以从原始训练样本集中选择最优的子集进行 KNN 的寻找。 这类方法主要包括 Condensing算法、WilSon 的 Editing 算法和 Devijver 的 MultiEdit 算法，Kuncheva 使用 遗传算法 在这方面也进行了一些研究。 
  > 2. 加快搜索 k 近邻 ： 构造交叉索引表，根据划分的空间是否有混叠可以分为 Clipping 和 Overlapping 两种。前者划分空间没有重叠，其代表就是 k-d 树；后者划分空间相互有交叠，其代表为 R 树。 

- KNN 计算量过大问题的解决办法？

  >  目前常用的解决方法是事先**对已知样本点进行剪辑，事先去除对分类作用不大的样本** 
  >
  >  **分组方式**：将样本集按近邻关系分解成组，给出每组的质心得位置，以质心作为代表点，和位置样本计算距离，选出距离最近的一个或若干个组，再在组的范围内应用一般KNN算法，由于并不是将位置样本与所有样本计算距离，故该计算可减少计算量，但不能减少存储量。

- 什么是欧式距离和曼哈顿距离？

  > 见 $L_p$ 距离

- 为什么用欧式距离不用曼哈顿距离？

  >  不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。 

- 如何选择合适的距离衡量？

   > 众所周知当变量数越多，欧式距离的区分能力就越差。
   >
   > 变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。 
   >
   >  一般都会进行归一化，如将数值范围处理到 0 到 1 之间，以此来保证每个特征是同等重要的。 

- KNN中k如何选取？  

  > - 如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，模型整体地复杂度会上升，如果邻近的实例点恰巧是噪声，预测就会出错，容易发生过拟合。 
  > - 如果选择较大的k值，就相当于用较大的邻域中的训练实例进行预测，容易欠拟合，使预测不准确。
  > - k值小的时候 近似误差小，估计误差大。 k值大 近似误差大，估计误差小。反映了对近似误差与估计误差之间的权衡
  > - 如果k=N，则所有预测结果都为训练集实例中最多的分类。
  > - 在应用中，k值一般取一个较小的数值，通常使用**交叉验证**选取最优$k$，算是超参数。
  > - 二分类问题，$k$ 选择奇数有助于避免平票。

- 在KNN的样本搜索中，如何进行高效的匹配查找？

  > 1. 线性扫描(数据多时，效率低) 
  > 2. 构建数据索引——Clipping和Overlapping两种。前者划分的空间没有重叠，如k-d树；后者划分的空间相互交叠，如R树。 

- 那什么是KD树？怎么构建的？ 

  >  kd树是对数据点在k维空间中划分的一种数据结构，主要用于多维空间关键数据的搜索。本质上，kd树就是一种平衡二叉树。 
  >
  >  **构建过程**：KDTree的构建是一个递归的过程，不断地对k维空间进行切分，生成子结点。在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）。这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。

- KD树建立过程中切分维度的顺序是否可以优化？ 

  > 先对各个维度计算方差，选取最大方差的维度作为候选划分维度(方差越大，表示此维度上数据越分散)；对split维度上的值进行排序，选取中间的点为node-data；按照split维度的node-data对空间进行一次划分；对上述子空间递归以上操作，直到空间只包含一个数据点。分而治之，且循环选取坐标轴。从方差大的维度来逐步切分，可以取得更好的切分效果及树的平衡性。

- KD树每一次继续切分都要计算该子区间在需切分维度上的中值，计算量很大,有什么方法可以对其进行优化？ **TODO**

  >  在构建KD树前，依据每一维度先排序，在之后的切分中直接使用。 

- 能简单介绍一下KD树的查找，以及增、删、改的实现流程吗？ **TODO**

  > 先二叉查找，找到候选最近点；沿着路径进行回溯，画圆，是否父节点平面交割，以判断是否需要进入另一个平面进行查找；依次回溯，画圆，寻找最近点。 

- KDTree的k与KNN的k的区别？

  >  KNN中的K代表最近的K个样本，KD树中的K代表样本特征的维数。 

- 能否大幅减少训练样本量，同时又保持分类精度？**TODO**

  >  在精度下降有限的前提下，降低维度，减少算法的计算量 
  >
  >  **浓缩技术(condensing)**： 基于 Fuzzy ART 的 K- 最近邻分类改进算法,该算法用模糊自适应共振理论(Fuzzy ART)对 K- 最近邻的训练样本集进行浓缩, 以改善 K- 最近邻的计算速度。（**就是对样本的维度进行简化，提取出特征明显的维度进行计算**）
  >
  >  **CHI 概率统计方法**进行初步特征提取和模式聚合；（如果某一维在各个类别中取值基本相同, 那么此维对于文本分类的贡献率就相对较低, 如果在各个类别中取值有较大的差异, 那么就具有较强的文本分类能力, 而方差正好是反应变量分布均匀状态的主要指标。）可引入CLA(Classifier’s Local Accuracy)技术进行分类可信度分析以实现两种算法的融合 
  >
  >  **编辑技术(editing)**   

- K-Means与KNN有什么区别 ？

  >  K-means是聚类算法，KNN用来分类和回归 

- KNN算法怎样处理缺失值？

  > KNN算法缺失数据需要额外处理，一般根据训练集数据的相似性填充，即先找到与缺失数据最近的k个邻居，取其均值、中位数、众数等来填充缺失值。

- KNN Regression的缺点是什么？

  >  第一个缺点是对于样本少的区域容易overfitting。在边界处有很大的bias。 
  >
  >  曲线弯弯曲曲，不太连续。 
  >
  > 这一点带来的问题就是比如预测房价，80平米和81平米的预测值可能会因为不连续导致差异较大，这样让人感觉预测效果并不可信。搜房什么的肯定不会用这种方法，因为不好对客户解释。 

- 如何对KNN的训练样本进行维护？

  > 对训练样本库进行维护以满足 KNN 算法的需要，包括对训练样本库中的样本进行添加或删除。 
  >
  > 对样本库的维护并不是简单的增加删除样本，而是可采用适当的办法来保证空间的大小，如符合某种条件的样本可以加入数据库中，同时可以对数据库库中已有符合某种条件的样本进行删除。从而保证训练样本库中的样本提供 KNN 算法所需要的相对均匀的特征空间。 

- 什么是 限定半径最近邻算法？

  > 在sklearn中的**RadiusNeighborsClassifier**
  >
  > 有时候我们会遇到这样的问题，即样本中某系类别的样本非常的少，甚至少于K，这导致稀有类别样本在找K个最近邻的时候，会把距离其实较远的其他样本考虑进来，而导致预测不准确。为了解决这个问题，我们限定最近邻的一个最大距离，也就是说，我们只在一个距离范围内搜索所有的最近邻，这避免了上述问题。这个距离我们一般称为限定半径。

- 什么是 最近质心算法是什么？

  > 这个算法比KNN还简单。 它首先把样本按输出类别归类。对于第 L类的 $C_l$ 个样本。它会对这 $C_l$ 个样本的n维特征中每一维特征求平均值，最终该类别所有维度的n个平均值形成所谓的质心点。对于样本中的所有出现的类别，每个类别会最终得到一个质心点。当我们做预测时，仅仅需要比较预测样本和这些质心的距离，最小的距离对于的质心类别即为预测的类别。这个算法通常用在文本分类处理上。 

- 搜索算法如何选择？

  > 如果样本少特征也少，使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用"auto"建树时间会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。 