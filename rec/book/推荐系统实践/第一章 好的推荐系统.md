## 1.1 什么是推荐系统

推荐系统的任务就是**联系用户和信息**

- 社会化推荐（朋友推荐）
- 基于内容的推荐（内容上相似）
- 基于协同过滤的推荐

解决信息过载代表性的解决方案是**分类目录**（雅虎）和**搜索引擎**（谷歌）

搜索引擎——> 有明确目的

推荐系统——> 无明确目的

推荐系统能更好地发觉物品的**长尾**

> 长尾商品是指虽然不热门，但是其总销售额却是一个不可小觑的数字，往往代表了一小部分用户的个性化需求

## 1.3 推荐系统测评

好的推荐系统不仅仅能够准确预测用户的行为，而且能够拓展用户的视野，帮助用户发现那些他们可能会感兴趣，但却不那么容易发现的东西。

实验方法分为**离线实验**和**在线实验**

> **在线AB测试**：随机分组，不同的组采用不同的推荐算法
>
> 生产中一般不会用AB实验来测试所有的算法，而只是用它测试那些在离线实验和用户调查中表现很好的算法，因为其周期比较长，必须长期实验才能得到可靠的结果。
>
> 不同层之间进行AB测试，必须**切分流量**，排除互相干扰



一个新算法的上线

1. 离线指标
2. 用户调查
3. AB测试



**评测指标**

- 用户满意度

  很少进行离线计算，基本用**用户调查**和**在线实验**

  从不同的侧面询问用户对结果的不同感受

  添加反馈按钮（如知乎、淘宝）

  可以用**点击率**、**用户停留时间**和**转化率**等指标度量用户的满意度





- 预测准确度

  > 预测准确度是推荐系统领域最重要的指标

  - 评分预测

    $r_{ui}$ 是用户 u 对物品 i 的实际评分

    $\hat{r}_{\mathrm{ui}}$ 是推荐系统的预测评分

    **均方根误差**： $\mathrm{RMSE}=\frac{\sqrt{\sum_{u , i \in T}\left(r_{u}-\hat{r}_{ui}\right)^{2}}}{|T|}$

    **均方绝对误差**：$\mathrm{MAE}=\frac{\sum_{u, i \in T}\left|r_{u i}-\hat{r}_{u i}\right|}{|T|}$

    >  T：记录条数

    **召回率（查全率）**： Recall $=\frac{\sum_{u \in U}|R(u) \cap T(u)|}{\sum_{u \in U}|T(u)|}$

    推荐了100条新闻，其中10条用户产生了点击，而用户最终在平台上总共点击了200条新闻，那么召回率为10 / 200 = 0.05

    **准确率（查准率）**：Precision $=\frac{\sum_{u \in U}|R(u) \cap T(u)|}{\sum_{u \in U}|R(u)|}$ 
    
    给用户推荐了100条新闻，其中10条用户产生了点击，那么准确率为10/100 = 0.1
    
    

    > R(u) 根据用户在训练集上的行为给用户作出的推荐列表（预测的）
    >
    > T(u) 用户在测试集上的行为列表（真实的）
    >
    > TopN推荐的预测准确率一般通过 **准确率/召回率**，一般会取不同的推荐列表长度 N，计算出一组 ，并画出曲线
    
    

- 覆盖率

  描述一个推荐系统对物品**长尾**的发掘能力

  Coverage $=\frac{\left|\cup_{u \in U} R(u)\right|}{|I|}$ 能推荐出来的物品占总物品集合的比例

  通过研究物品在推荐列表中出现次数的分布描述推荐系统挖掘长尾的能力

  > 是一个内容提供商会关系的指标

  **信息熵** $H=-\sum_{i=1}^{n} p(i) \log p(i)$

  https://blog.csdn.net/u013853733/article/details/98514129

  **基尼系数**

​		P44



- 多样性
- 新颖性
- 惊喜度
- 信任度
- 实时性
- 健壮性
- 商业指标







## Others

- 怎么理解惩罚项（Penalization）？

  > 惩罚的核心目的是限制参数空间的大小以降低模型的复杂度，来降低过拟合
  >
  > 在统计中一般叫正则化（Regularization）
  >
  > 是一种为了减少测试误差的行为，有时候会增加训练误差

- L1 惩罚项（正则化） 和 L2 惩罚项（正则化） ？

  > $\|x\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}$
  >
  > $\Omega(w)=\|w\|_{1}=\sum_{i}\left|w_{i}\right|$  L1更适合用于特征选择
  >
  > 更新 $w$  时，若为正数，则每次更新会减去一个常数。若为负数，则每次更新会加上一个常数，所以很容易产生特征的系数为 0 的情况，表示该特征不会对结果有任何影响，因此L1正则化会让特征变得稀疏，起到特征选择的作用。
  >
  > 
  >
  > $\Omega(w)=\|w\|_{2}^{2}=\sum_{i} w_{i}^{2}$ L2更适合用于防止模型过拟合
  >
  > 更新 $w$ 时，会对特征系数进行一个比例的缩放，这会让系数趋向变小而不会变为 0，所以L2正则化会让模型变得更简单，防止过拟合。

- dropout 正则化？

https://www.jianshu.com/p/569efedf6985

- earlystopping 正则化？

- 推荐系统中用 F值 做指标吗？

  >

- 推荐系统怎样做模型融合？





