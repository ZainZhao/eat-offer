基于BPR、pair-wise的软化



- 结合之前的SPGAN来压缩Embedding的维度，因为他本来就是用来



## 九月 第二周

20200910-20200921

- 实验（G_2D、G_v、D）
- 压缩比例（看paper中）
- 时间推断GPU&CPU（看paper中）、测时间的时候需要预热一下
- PPT做一下原子模型的解释



D—2D改进

- 绝对值
- 看一下没一步的loss
- 看一下对于 120 维度



重新测一下 100 和 120 的raw



**20200922-20200929**

- 做完全部原子实验
- 整理各原子实验的说法
- 想进一步融合与改进
- SBPR
- selective
- 测试一下实验对比



20201007

- G_2G_SD  加上ratio和epoch 重新跑
- G_2G_TD  出结果



> 发现
>
> - 目前有一篇使用GAN的，但是他使用了其它信息，倾向于迁移学习
>
> - 多层的效果不好，可能是数据量的原因，看一下损失下降的情况
>
> - 有些比较前沿模型，需要很大的内存进行训练很多情况下不是因为model很大，而是在做数据时需要做一些装换，导致空间复杂度很高
>
> - 重心就可以往“做Light的Gan推荐”
>
> - 如果负样本的抽样说不定能增加模型上的contribution
>
>   
>
> - itembased和userbased之间相互迁移
>
> 
>
> - 类似于**自蒸馏压缩**
>
> 
>
> - CFGAN虽然缓解了一些矛盾，但是在训练后期，性能还是会下降
>
> 
>
> - 相当于是使用KD来改变masking的方式
>
> 
>
> - **还是用一下带温度的逼近**
> - 各loss之间相加最好还是正则化一下
>
> 
>
> - KD策略的通用性
>
> 
>
> - 加一点对推荐相关的假设进去
>
> 
>
> - 有些方法加在大的model上没作用，但反过来可以替升小的模型
>
> - CFGAN不是很稳定
>
> - **KL散度**
>
> - 还是要分析一下性能的缺陷在哪里
>
>   
>
> - 主要还是判别器跟不上生成器的节奏？
>
> 
>
> - 之前的model对于混合蒸馏的方式的理由没有基于GAN的充分,因为GAN本来就是对抗的，并且CFGAN还需要平衡生成器和判别器的能力
>
> 
>
> - **用不同的蒸馏方式在G和D两边进行对抗**
> - **压缩GAN的话，无论哪一边都对导致性能的不平衡**
>
> 
>
> - 武士对抗的例子
>
> 
>
> - sample其实不影响D和G，可能同时对他们进行了加强，所以说不定可以与其他方式结合，性能会更好
>
> 
>
> - 时间还是和设备有关系、核数，并发并行
>
> 
>
> - 后两个指标可能是跟取值的方式有关（因为是max(pre)）
>
> 
>
> - 可能G_kd_v也是需要后几个batch再逼近
>
> 
>
> - 不同的KD在不同的epoch加
>
> - 单独的G不好，不一定搭配的效果不好
>
> - 还是得看一看GAN+KD的paper
>
>   问题
>
> - 能不能用SBPR来增强BPR,那为什么不直接使用SBPR呢
>
>   



